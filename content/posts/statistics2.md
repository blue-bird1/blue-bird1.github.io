---
title: 统计学2:现代概率
date: 2019-01-07T21:20:23+08:00
lastmod: 2019-01-07T21:20:23+08:00
author: bluebird
categories: ["统计学"]
tags: ["学习笔记"]
---

####  统计学2: 现代概率

现代概率的一大特征是概率不确定性.古代概率研究的骰子可以认为每面的概率是1/6, 但是统计天气的时候, 就没有理由认为晴天和雨天的概率都是1/2. 

<!--more-->
概率函数在骰子只是简单的$\rho(任意一个数)= \frac{1}{6}$  . 在复杂的现实情况使用简单的函数是无法描述的.



#### 偏差和方差

如果怀疑赌场的骰子大小游戏有问题, 要怎么去验证它这个问题. 一种思路是认为如果1,2,3,4,5,6出现的概率不一样 就是有问题. 换种等价方法描述就是 骰子期望$E(X)=3.5$ . 这个骰子均值也应该是3.5.

那么衡量离理想骰子距离也有几种方法 除了最简单的均值.

偏差 所有发生事件 $X = [x_1, x_2 ... x_n]$  偏差 $ =  \sum_n E(X) - x_n$

例如连续出现5个6    $6*5  - 3.5*5 = 2.5*5$     但是连续5个6并不是现实的小概率事件.  如果发生连续5个6就认为骰子有问题显然是不行的.  而且偏差求的是平均, 两边偏差会抵消  $[3,4,3,4]$ 这种的偏差也是0,

方差则是偏差的平方 $ \sum_n (E(X) - x_n)^2$ 方差是一个衡量离散度的标准  



#### 大数定律

我们已经知道统计会出现偏差, 那么如何确定偏差是否是骰子本身的问题呢. 初中就有教, 统计次数越大, 越可能接近均值. 

用数学表达就是伯努利大数律 任意给定两个数 $\epsilon$, $\eta$ .含义分别是离均值的距离和离均值的概率. 事件总值是$X$, 存在一个抽取次数$N$ 使得 $\rho(| \frac{X}{N}|>\epsilon ) < \eta$ 

伯努利证明时并没有方差, 但是他还是用概率的方法证明了. 

> 证明参考 数理统计学简史 第一章注3 

现在我们知道只是切比雪夫不等式的一个推论. 

$\rho(|X-E(X)| > b\sigma) \geq \frac{1}{b^2}$   $\sigma$ 是方差的平方根 也叫标准差 X是随机变量

这个不等式之前还有一个马可夫不等式

$P(X\geq a) \leq  \frac{E(x)}{a}$

这个就比较容易理解了. 如果a取值范围是在x之间. 

将$Y=(X-\epsilon)^2$  $a=(kσ)^2$代入马可夫不等式

$\rho((X-\epsilon)^2)\geq kσ^2)\leq \frac{E(X-\epsilon)^2}{kσ^2)} = \frac{σ^2}{kσ^2} =\frac{1}{k^2} $



#### 概率函数和累计分布函数

如果随机变量是X, 

概率函数$\rho$

$\rho(x) = x的概率$ 



硬币抛出多少次正面的概率分布. 因为只有两个可能性的情况太常见,也被称为二次分布.

n是总数量 k是正的数量 p是正的概率 二项式分布 $b(n, k, p ) = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k} $

前面组合公式很显然, 后面$p^k$是抛出这么多次正面的概率  $(1-p)^{n-k}$则相反 

用二项分布可以计算出硬币概率分布  $ \rho(k)=\frac{n!}{k!(n-k)!}\frac{1}{2}^k\frac{1}{2}^{n-k}$

它的期望是$E(X) = np$  因为二项分布是n次相同实验组合的. 不管抛多少次 期望都是正面的几率$p$ 再乘以总数量

方差是$\sigma^2 = np(1-p)$



累计分布函数f是

$\rho(X \leq x) =f(x)$

分布函数是概率函数的积分, 所以最终将变成1, 



#### 贝叶斯公式

我们在概率定义知道 独立变量a,b有  $P(b|a) = P(a,b)/P(a)$ 和$P(a,b) = P(a|b)*P(b)$ 

将后项带入前项 得$P(b|a) =  P(a|b)*P(b)/P(a)$



独立变量, 也就是两个变量不相互影响. 天气自然是和骰子没什么关系, 但是骰子掷出双数和骰子掷出6自然是有关系. 在概率学上的定义是协方差, X, Y的期望值是$\mu$和$v$

$Cov(X, Y) = E((X-\mu)(Y-v))$

为什么这么定义呢. 这两个期望分开定义 都是0.  现在放到一起如果不是0, 说明存在相互影响. 期望值的大小也代表影响的方向



#### 二项分布逼近

高斯分布可谓是第一重要的分布  它其实是二项分布的逼近, 然后得出的函数.

在历史上首先研究这个问题的是棣莫弗, 他一开始研究的对象 是$b(2m, 1/2, m)$ 

m是2m的中项 然后  $b(m)/b(m+d)$ 中项和偏离中项的关系

然后他的朋友斯特林出现, 并使用斯特林公式算了一下

$b(2m, 1/2. m) \approx \frac{2}{mn}$   

  $\frac{b(m+d)}{bm}\approx e^-\frac{2d^2}{n}$ 

$b(m+d) \approx  \frac{2}{\sqrt{2πn}}  e^-\frac{2d^2}{n} $

>  使用上式的结果，并在二项概率累加求和的过程中近似的使用定积分代替求和，得到
>
> $p_d  \approx  \sum_{-d\leq  m-i\leq d} \frac{2}{\sqrt{2πn}} e^{-2\frac{d}{\sqrt{n}}^2} \approx  \frac{2}{\sqrt{2π}}\int^\frac{d}{\sqrt{n}}_{-\frac{d}{\sqrt{n}}} e^{-2x^2}dx  = \frac{1}{\sqrt{2π}}\int^\frac{2d}{\sqrt{n}}_{-\frac{2d}{\sqrt{n}}} e^{\frac{x^2}{-2}}dx $  

将d换成$\frac{c}{\sqrt{n}}$ 可得$ \frac{1}{\sqrt{2π}}\int^{2c}_{-2c} e^{-{\frac{x^2}{2}}}dx $  熟悉的高斯分布

这个公式有什么特殊之处呢 $\frac{1}{\sqrt{2π}}\int^{\infty}_{-\infty} e^{-{\frac{x^2}{2}}}dx =1 ​$

$\int^{\infty}_{-\infty} e^{-{\frac{x^2}{2}}}dx$  也叫高斯积分 

#### 参考

数理统计简史 

程序员的数学2

概率论与梳理统计 

https://www.qiujiawei.com/shadow-1/
